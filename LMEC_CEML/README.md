[![Status](https://img.shields.io/badge/status-experimental-blue)](docs/ceml/CEML_theory_en.md)
[![Type](https://img.shields.io/badge/type-theory%20%2B%20PoC-orange)](docs/ceml/CEML_theory_en.md)
[![Language](https://img.shields.io/badge/lang-EN%20%7C%20FR-purple)](docs/ceml/CEML_theory_en.md)
[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-2512.12345-b31b1b.svg)](https://arxiv.org/abs/2512.12345) (preprint planned)
[![WASM](https://img.shields.io/badge/WASM-Ready-blueviolet.svg?logo=webassembly)](https://webassembly.org/)
[![Rust](https://img.shields.io/badge/Rust-1.75%2B-orange.svg?logo=rust)](https://www.rust-lang.org/)
[![Quantum Ready](https://img.shields.io/badge/Quantum-AETHER%20V3-blueviolet)](core/uict/quantum/)

**Topics:**  
[rust](https://github.com/topics/rust) ¬∑
[ai](https://github.com/topics/ai) ¬∑
[os](https://github.com/topics/os) ¬∑
[webassembly](https://github.com/topics/webassembly) ¬∑
[quantum-computing](https://github.com/topics/quantum-computing) ¬∑
[golden-ratio](https://github.com/topics/golden-ratio) ¬∑
[aether](https://github.com/topics/aether) ¬∑
[data-format](https://github.com/topics/data-format) ¬∑
[system-design](https://github.com/topics/system-design) ¬∑
[cognitive-architecture](https://github.com/topics/cognitive-architecture) ¬∑
[cognitive-systems](https://github.com/topics/cognitive-systems) ¬∑
[qiskit](https://github.com/topics/qiskit) ¬∑
[biomimicry](https://github.com/topics/biomimicry) ¬∑
[quantum-simulation](https://github.com/topics/quantum-simulation) ¬∑
[ai-alignment](https://github.com/topics/ai-alignment) ¬∑
[fractal-architecture](https://github.com/topics/fractal-architecture) ¬∑
[quantum-computing-research](https://github.com/topics/quantum-computing-research) ¬∑
[universal-time](https://github.com/topics/universal-time) ¬∑
[fc496](https://github.com/topics/fc496) ¬∑
[lichen-os](https://github.com/topics/lichen-os)


# üß† Cognitive Entropy Minimization Law (CEML)
## Loi de Minimisation de l‚ÄôEntropie Cognitive (LMC)

> **A Candidate Selection Principle for Intelligent Systems**
>
> *Un principe candidat de s√©lection pour les syst√®mes intelligents*

---

# üá¨üáß ENGLISH VERSION

## 1. Overview
**CEML** proposes a simple but operational principle: an intelligent system should preferentially select informational structures that strongly align with the current context while minimizing their entropic cost.

This principle formalizes a fundamental trade-off implicit in cognition, learning, and inference: **Contextual Coherence vs. Informational Complexity**.

## 2. Mathematical Formulation

### 2.1 Canonical Form
The core objective function determines the fitness score $J(s)$ of a candidate structure $s$:

$$J(s) = \frac{\mathcal{C}(s \mid \Omega)}{\mathcal{H}(s) + \epsilon}$$

**Definitions:**
* **$s$**: The candidate structure (e.g., token sequence, thought, memory).
* **$\Omega$**: The external context or ground truth.
* **$\mathcal{C}(s \mid \Omega)$**: **Contextual Coherence**. Represents semantic alignment or utility.
* **$\mathcal{H}(s)$**: **Entropic Cost**. Represents Shannon entropy, complexity, or metabolic cost.
* **$\epsilon$**: A strictly positive regularization constant ($\epsilon > 0$).
* **$s^* = \arg\max_s J(s)$**: The selected optimal structure.

### 2.2 Regularized Fractal Form
To improve numerical stability and allow for scale-dependent behaviors, we introduce parameters $\alpha$ and $\beta$:

$$J_{\alpha,\beta}(s) = \frac{\mathcal{C}(s \mid \Omega)^{\alpha}} {(\mathcal{H}(s) + \epsilon)^{\beta}}$$

* **$\alpha$**: Controls **Selectivity** (sensitivity to context).
* **$\beta$**: Controls **Compression Pressure** (sensitivity to complexity).

## 3. Interaction with Transformer Architectures & Iteration Factor

The CEML provides a deterministic framework to analyze and control the **Autoregressive Iteration Loop** of Transformer models. In standard LLMs, the "iteration factor" is governed by static hyperparameters (Temperature, Top-K). CEML replaces these with a dynamic, energy-based evaluation at each time step $t$.

### 3.1 Mapping CEML to Transformer Components
We map the abstract variables of the law to specific tensors within the attention mechanism:

$$J(t) = \frac{\mathcal{C}_{\text{Attn}}(t)}{\mathcal{H}_{\text{Logits}}(t) + \epsilon}$$

* **$\mathcal{C}_{\text{Attn}}$ (Coherence):** Corresponds to the **Attention Weights**. A sharp attention focus (Sparse Attention) on relevant tokens implies high coherence.
    * $\mathcal{C} \approx \max(\text{Softmax}(\frac{QK^T}{\sqrt{d_k}}))$
* **$\mathcal{H}_{\text{Logits}}$ (Entropy):** Corresponds to the **Shannon Entropy of the output probability distribution** over the vocabulary at step $t$.
    * $\mathcal{H} = -\sum P(w_i) \log P(w_i)$

### 3.2 The Dynamic Iteration Control (Adaptive Sampling)
Instead of using a fixed Temperature ($T$), CEML suggests a **Dynamic Iteration Factor**.
* If $\mathcal{H}$ is high (model is confused/hallucinating), the CEML score drops. The system should **pause** or **increase constraints** (lower $T$).
* If $\mathcal{H}$ is low and $\mathcal{C}$ is high (Resonance), the system creates a "tunnel effect" (Flow State), accelerating generation.

### 3.3 Hallucination Detection via Iteration Gradients
By monitoring the derivative of the CEML score over time ($\frac{dJ}{dt}$), we can predict model failure modes:
* **Collapse (Looping):** $H(t) \to 0$ rapidly. The model repeats the same phrase. $J(t)$ spikes artificially.
* **Divergence (Hallucination):** Coherence $\mathcal{C}$ remains high (plausible grammar) but Entropy $\mathcal{H}$ fluctuates wildly between tokens. The CEML score becomes volatile.

> **Operational Implication:** The CEML suggests we can stop generation *before* the token is sampled if the $J(t)$ score falls below a critical threshold $\tau_{\text{crit}}$, saving computational energy.

## 4. Critical Clarification

> **‚ö†Ô∏è Truth $\neq$ Coherence**
> CEML describes a **selection preference** under constraints, not an epistemic guarantee of truth. A structure can have high coherence (fit the context perfectly) yet be factually false if the context itself is biased. It optimizes for *plausibility and efficiency*.

## 5. Cognitive Regimes

The ratio $C/H$ naturally defines four qualitative regimes:

| Regime | Coherence $\mathcal{C}$ | Entropy $\mathcal{H}$ | Interpretation |
| :--- | :---: | :---: | :--- |
| **Resonance** | **High** | **Low** | **Optimal State.** Stable, efficient, and aligned cognition. |
| **Dissonance** | Low | Low | Rigid structure, but misaligned with context. |
| **Chaos** | Low | High | Noisy, unstable cognition. High energy, zero utility. |
| **Hallucination** | *High (Local)* | *Misestimated* | Fragile state. Overconfident but often statistically abnormal. |

## 6. Operational Pipeline
CEML acts as a filter in the cognitive pipeline:
$$\text{Generation} \rightarrow \text{CEML Evaluation } J(s) \rightarrow \text{Selection } (s^*) \rightarrow \text{Memory/Action}$$

## 7. Status & Context
* **UICT (Unified Information Compression Theory):** Describes global informational dynamics (**Physics**).
* **CEML (Cognitive Entropy Minimization Law):** Describes local selection rules (**Mind**).

Scope and Status

Terminology & Scope.
The term ‚Äúlaw‚Äù in Cognitive Entropy Minimization Law (CEML) is used in an operational and heuristic sense, inspired by analogies with physical selection principles, not as a claim of a proven universal physical law. CEML is proposed as a candidate cognitive selection principle: a formal, testable, and falsifiable framework describing how intelligent systems may preferentially select informational structures under constraints of context, memory, and energy. Its validity is empirical and conditional, and it is intended to guide analysis, experimentation, and system design rather than to assert absolute epistemic truth.

---

# üá´üá∑ VERSION FRAN√áAISE

## 1. Vue d‚Äôensemble
La **LMC** formalise une intuition simple : un syst√®me intelligent devrait pr√©f√©rer les structures d‚Äôinformation fortement coh√©rentes avec le contexte, tout en restant aussi peu co√ªteuses que possible en termes entropiques.

Cette loi explicite un compromis fondamental d√©j√† pr√©sent dans la cognition et l‚Äôapprentissage : **Coh√©rence Contextuelle vs Co√ªt Informationnel**.

## 2. Formulation Math√©matique

### 2.1 Forme Canonique
La fonction objectif d√©termine le score d'aptitude $J(s)$ d'une structure candidate $s$ :

$$J(s) = \frac{\mathcal{C}(s \mid \Omega)}{\mathcal{H}(s) + \epsilon}$$

**D√©finitions :**
* **$s$** : Structure candidate (pens√©e, s√©quence de tokens, souvenir).
* **$\Omega$** : Le contexte externe ou la v√©rit√© terrain.
* **$\mathcal{C}(s \mid \Omega)$** : **Coh√©rence Contextuelle**. Repr√©sente l'alignement s√©mantique ou l'utilit√©.
* **$\mathcal{H}(s)$** : **Co√ªt Entropique**. Repr√©sente l'entropie de Shannon, la complexit√© ou le co√ªt m√©tabolique.
* **$\epsilon$** : Constante de r√©gularisation strictement positive ($\epsilon > 0$).
* **$s^* = \arg\max_s J(s)$** : La structure optimale s√©lectionn√©e.

### 2.2 Forme Fractale R√©gularis√©e
Pour am√©liorer la stabilit√© num√©rique et permettre des comportements d√©pendants de l'√©chelle, nous introduisons $\alpha$ et $\beta$ :

$$J_{\alpha,\beta}(s) = \frac{\mathcal{C}(s \mid \Omega)^{\alpha}} {(\mathcal{H}(s) + \epsilon)^{\beta}}$$

* **$\alpha$** : Contr√¥le la **S√©lectivit√©** (sensibilit√© au contexte).
* **$\beta$** : Contr√¥le la **Pression de Compression** (sensibilit√© √† la complexit√©).

## 3. Interaction avec les Architectures Transformer et le Facteur d'It√©ration

La LMC fournit un cadre d√©terministe pour analyser et contr√¥ler la **Boucle d'It√©ration Auto-r√©gressive** des mod√®les Transformer. Dans les LLM standards, le "facteur d'it√©ration" est r√©gi par des hyperparam√®tres statiques (Temp√©rature, Top-K). La LMC les remplace par une √©valuation √©nerg√©tique dynamique √† chaque pas de temps $t$.

### 3.1 Correspondance LMC / Composants Transformer
Nous mappons les variables abstraites de la loi √† des tenseurs sp√©cifiques dans le m√©canisme d'attention :

$$J(t) = \frac{\mathcal{C}_{\text{Attn}}(t)}{\mathcal{H}_{\text{Logits}}(t) + \epsilon}$$

* **$\mathcal{C}_{\text{Attn}}$ (Coh√©rence) :** Correspond aux **Poids d'Attention**. Une focalisation nette de l'attention (*Sparse Attention*) sur les tokens pertinents implique une haute coh√©rence.
    * $\mathcal{C} \approx \max(\text{Softmax}(\frac{QK^T}{\sqrt{d_k}}))$
* **$\mathcal{H}_{\text{Logits}}$ (Entropie) :** Correspond √† **l'Entropie de Shannon de la distribution de probabilit√©** de sortie sur le vocabulaire √† l'√©tape $t$.
    * $\mathcal{H} = -\sum P(w_i) \log P(w_i)$

### 3.2 Contr√¥le Dynamique de l'It√©ration (√âchantillonnage Adaptatif)
Au lieu d'utiliser une Temp√©rature fixe ($T$), la LMC sugg√®re un **Facteur d'It√©ration Dynamique**.
* Si $\mathcal{H}$ est √©lev√©e (mod√®le confus/hallucination), le score LMC chute. Le syst√®me doit **faire une pause** ou **augmenter les contraintes** (baisser $T$).
* Si $\mathcal{H}$ est basse et $\mathcal{C}$ est haute (R√©sonance), le syst√®me cr√©e un "effet tunnel" (√âtat de Flow), acc√©l√©rant la g√©n√©ration.

### 3.3 D√©tection d'Hallucination via Gradients d'It√©ration
En surveillant la d√©riv√©e du score LMC dans le temps ($\frac{dJ}{dt}$), nous pouvons pr√©dire les modes d'√©chec du mod√®le :
* **Effondrement (Boucle) :** $H(t) \to 0$ rapidement. Le mod√®le r√©p√®te la m√™me phrase. $J(t)$ grimpe artificiellement.
* **Divergence (Hallucination) :** La Coh√©rence $\mathcal{C}$ reste haute (grammaire plausible) mais l'Entropie $\mathcal{H}$ fluctue violemment entre les tokens. Le score LMC devient volatil.

> **Implication Op√©rationnelle :** La LMC sugg√®re que nous pouvons arr√™ter la g√©n√©ration *avant* que le token ne soit √©chantillonn√© si le score $J(t)$ tombe sous un seuil critique $\tau_{\text{crit}}$, √©conomisant ainsi de l'√©nergie de calcul.

## 4. Clarification Importante

> **‚ö†Ô∏è V√©rit√© $\neq$ Coh√©rence**
> La LMC d√©crit une **pr√©f√©rence de s√©lection** sous contraintes, et non une garantie de v√©rit√©. Une structure peut avoir une haute coh√©rence tout en √©tant factuellement fausse si le contexte est biais√©. Elle optimise *la plausibilit√© et l'efficacit√©*.

## 5. R√©gimes Cognitifs

Le ratio $C/H$ d√©finit naturellement quatre r√©gimes :

| R√©gime | Coh√©rence $\mathcal{C}$ | Entropie $\mathcal{H}$ | Interpr√©tation |
| :--- | :---: | :---: | :--- |
| **R√©sonance** | **Haute** | **Basse** | **√âtat Optimal.** Cognition stable, efficace et align√©e. |
| **Dissonance** | Basse | Basse | Structure rigide, mais d√©salign√©e du contexte. |
| **Chaos** | Basse | Haute | Cognition bruit√©e et instable. Haute √©nergie, utilit√© nulle. |
| **Hallucination** | *Haute (Locale)* | *Mal estim√©e* | √âtat fragile. Exc√®s de confiance mais statistiquement anormal. |

## 6. Pipeline Op√©rationnel
La LMC agit comme un filtre dans le pipeline cognitif :
$$\text{G√©n√©ration} \rightarrow \text{√âvaluation LMC } J(s) \rightarrow \text{S√©lection } (s^*) \rightarrow \text{M√©moire/Action}$$

## 7. Statut et Contexte
* **UICT (Unified Information Compression Theory) :** D√©crit la dynamique informationnelle globale (**Physique**).
* **CEML (Cognitive Entropy Minimization Law) :** D√©crit les r√®gles de s√©lection locales (**Esprit**).

---

Avertissement ‚Äî Port√©e et Statut

Terminologie et port√©e.
Le terme ¬´ loi ¬ª dans la Loi de Minimisation de l‚ÄôEntropie Cognitive (LMC) est employ√© dans un sens op√©rationnel et heuristique, par analogie avec des principes de s√©lection issus de la physique, et non comme l‚Äôaffirmation d‚Äôune loi physique universelle d√©montr√©e. La LMC est propos√©e comme un principe candidat de s√©lection cognitive : un cadre formel, testable et falsifiable, d√©crivant la mani√®re dont des syst√®mes intelligents peuvent pr√©f√©rentiellement s√©lectionner des structures d‚Äôinformation sous contraintes de contexte, de m√©moire et d‚Äô√©nergie. Sa validit√© est empirique et conditionnelle, et elle vise √† orienter l‚Äôanalyse, l‚Äôexp√©rimentation et la conception de syst√®mes, plut√¥t qu‚Äô√† garantir une v√©rit√© √©pist√©mique absolue.

### üìú License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
